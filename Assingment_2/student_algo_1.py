# -*- coding: utf-8 -*-
"""student_algo_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Peter5793/Data_analysis/blob/master/Assingment_2/student_algo_1.ipynb

# Student Binary Classification

Determine the classifiers for the student that may lead to a drop out
"""

#update the modules to be used
!pip install matplotlib

#importing the libraries to be used in the 
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import plotly.express as px
import numpy as np
import warnings
warnings.filterwarnings('ignore')

"""To partially automate the process, we shall connect to the google worksheet"""

#importing libraries to read from googlesheets
from google.colab import drive
drive.mount('/content/drive')

#install gspread pip for connection to google sheet
!pip install --upgrade -q gspread
!pip install gspread-dataframe

#authenticating the account 
import gspread
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
gc = gspread.authorize(GoogleCredentials.get_application_default())

#import the csv format of the dataset
student = gc.open_by_url('https://docs.google.com/spreadsheets/d/1PqGgqRiU4v5tnGqJgXhcfgZFzkqJgIhkimTx3-2oGA4/edit#gid=0')

from gspread_dataframe import get_as_dataframe, set_with_dataframe
ws = student.worksheet('Sheet1')
student_df = get_as_dataframe(ws)

#oberving the data frame
student_df.head()

#we do not need all the columns for the regression analysis therefore , we shall drop some of them 
# Intial droping of columns was made in the googles sheet
student_df.columns.to_list()

# looking at the shape of the data frame
shape = student_df.shape
print("{}".format(shape))

#looking at the columns with Null values
student_df.apply(lambda x: sum(x.isnull()), axis=0)

"""There are columns that have > 773 Nan values and we will perform some feature engineering in the columns to get data for > 28 years and > 30 years"""

student_df.drop(['Date of Birth','Source part 2','Source','Why drop or behind','Stage'], axis = 1, inplace  = True)

#looking at the columns again
student_df.apply(lambda x: sum(x.isnull()), axis = 0)

# lest us drop the missing values in the data frame
student_df.dropna(inplace=True)

student_df.apply(lambda x: sum(x.isnull()), axis = 0)

"""The dataset does not have any missing values, right now, we have dropped all the missing values.

This was done inorder to avoid any disturbance in our model.
"""

student_df.shape

"""# Explortory Analysis & Feature Engineering

Comparison between parameters in dropping out
"""

student_df.head(3)

#statistical sumamry of the data
student_df.describe()

# lookig at the columns and the index, so that we could select columns
for count, i in enumerate(student_df.columns):
  print("{} : {}".format(count, i))

scores_df = student_df.iloc[:,[8,9,10,11,12]]

mean_score = scores_df.mean(axis=1)

student_df['mean_score'] = mean_score

student_df.head()

#we shall rearrange the columns
student_df = student_df.iloc[:,[0, 1,2,3,4,5,6, 8,9,10,11,12,16,13,14]]

#this is the new dataframe that we shall use for our analysis
student_df.head()

#visulizing the age and gender distribution by country
fig = px.histogram(student_df, x='Age', nbins=10, color="Gender", 
                   title = "Student Distribution by age and Country",
                   labels = {#repalces default color mapping by value
                             "Age": "Age distribution",
                             "count":"Number of students"
                       
                   }, hover_name='Country',marginal = "rug")
fig.update_layout(# customize font and legend orientation & position
                  font_family = "Rockwell",
                  legend = dict(
                      orientation="h", y=1, yanchor='bottom', xanchor='center'
                  )
    
)
fig.show()

#visualize the scores and drop outs distribution
fig = px.histogram(student_df, x='mean_score',
                   color='Drop_out', nbins=10, hover_name = 'Country',marginal = 'rug',
                   title="Score Distribution and Drop_Outs",
                   color_discrete_map = { #replace the default color values
                                         'Drop_out = Yes':"cornflowerblue",
                                          'Drop_out = No':"blue"
                       
                   },
                   )
fig.update_layout(# customize fonts and the legends
                  font_family = "Rockwell",
                  legend = dict(
                      orientation="h", y=1, yanchor="bottom", x=0.5, xanchor="center"
                  ))
fig.show()

#visualize the scores and drop outs distribution by country
fig = px.histogram(student_df, x='mean_score',
                   color='Country', nbins=10, hover_name = 'Drop_out',marginal = 'rug',
                   title="Score Distribution and Drop_Outs",
                   
                   )
fig.update_layout(# customize fonts and the legends
                  font_family = "Rockwell",
                  legend = dict(
                      orientation="h", y=0.5, yanchor="bottom", x=1.5, xanchor="center"
                  ))
fig.show()

"""## Let us replace the variable values to Numerical form & display the Value Counts
This is to avoid disturbance in building the model
"""

student_df.head(3)

student_df['Drop_out'].replace('Yes', 1, inplace=True)
student_df['Drop_out'].replace('No', 0, inplace=True)

student_df['Drop_out'].value_counts()

student_df.Gender = student_df.Gender.map({'Male': 1, 'Female':0})

student_df['Overal_<60_?'].replace('Yes', 1, inplace=True)
student_df['Overal_<60_?'].replace('No', 0, inplace=True)

student_df['LevelStatus'].value_counts()

"""## Final DataFrame"""

student_df.head()

"""# Importing packagees for classification Algortithm"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

#split the data into training and test sets
X = student_df.iloc[:,[0,1, 7,8,9,10,11]]
Y = student_df.iloc[:, 14]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)

"""# Logistic Regression (LR)

Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable.

Mathematically, a logistic regression model predicts P(Y=1) as a function of X. It is one of the simplest ML algorithms that can be used for various classification problems such as spam detection, Diabetes prediction, cancer detection etc.


![image](data:image/jpeg;base64,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)
"""

model = LogisticRegression()
model.fit(X_train, y_train)

lr_prediction = model.predict(X_test)
print('Logistic regression accuracy {}'.format(metrics.accuracy_score(lr_prediction, y_test)))

print("y_predicted", lr_prediction)
print("y_test", y_test)

"""# Conclusion 
1. The drop out is heavily dependent on the Scores for prediction
2. The Logistic Regression algorithm gives us the maximum Accuracy (89% approx) compared to the other 3 Machine Learning Classification Algorithms.
"""