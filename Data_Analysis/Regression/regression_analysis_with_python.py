# -*- coding: utf-8 -*-
"""Regression Analysis with Python

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xakx3u_tdo8eGRD6aF_YJVrf7ImveSN0

# Practice Notebook: Regression Analysis with Python

## 1. Simple Linear Regression

#### <font color="blue">Pre-requisites</font>
"""

# Commented out IPython magic to ensure Python compatibility.
# The first thing that we will do is to import the required datasets
# ---
# OUR CODE GOES BELOW
#  

# Importing pandas for data manipulation
# 
import pandas as pd 

# Importing numpy for mathematical functions
import numpy as np

# Importing matplotlib for creating visualisations
# 
import matplotlib.pyplot as plt
# %matplotlib inline

"""#### <font color="blue">Example 1</font>

##### <font color="blue">Example</font>
"""

# Example
# --- 
# Question: Create a linear regression model to predict the output y given the input x from the dataset below.
# ---
# Dataset url = http://bit.ly/SimpleLinearRegresionDataset
# ---
# OUR CODE GOES BELOW
#

"""##### Step 1. Loading our Data """

# The first step that we will take is to import our dataset and store it in our dataframe named df
# --- 
# 
df = pd.read_csv('http://bit.ly/SimpleLinearRegresionDataset')
df.head()

"""##### Step 2. Checking the Data """

# We will then determine the size of our dataset using the keyword shape
# ---
# 
df.shape

# The preview our dataset using the head function()
# ---
#
df.head()

# We can also display the statistical summary of our dataset. 
# ---
# 
df.describe()

"""##### Step 3. Cleaning Our Data"""

# Lets confirm for missing values in our dataset. 
# We can already see from our summary that the count 
# of the y variable is not equal to x
# ---
#
df.isnull().sum()

# Observation 
# ---
# One record has a missing

# We can then resolve to propping all records with N/A from our dataset
# This would be for the reason that there won't be any significant effect in analysis.
# ---
#
df = df.dropna()

# Lets check again for missing values in our dataset
# ---
#
df.isnull().sum()

"""##### Step 4. Performing Exploratory Analysis"""

# We the perform exploratory analysis to understand more about our dataset
# ---
# In our case here, we will only find if there is any relationships between the two variables.
# ---
# 
df.plot(x='x', y='y', style='o',alpha = 0.25)
plt.title('x vs y')
plt.xlabel('x')
plt.ylabel('y')
plt.show()

# Observation
# ---
# Positive linear relation

"""##### Step 5. Implementation and Evaluation"""

# Data Preparation
# ---
# We now divide our data into "attributes" and "labels".
# a) Attributes are the independent variables. 
# They will be stored in the X variable, specifying "-1" as the range for columns 
# since we want our attribute set to contain all the columns 
# except the last one which is "scores".
# 
# b) Labels are the dependent variables. 
# They will be contained in the y variable, specifying 1 for the label colum 
# since the index for "scores" column is 1.
# 
# * NB: Column indexes start with 0, 1 being the second, 2 being third etc.
# - Because We are predicting the percentage score depending on hours studied, 
# our attribute set will consist of "hours", while the lable will be "score"
# 
# * As we will get to see in another example, we can also specify the column names to 
# select our attributes and label 
# ---
# 
X = df.iloc[:, :-1].values
y = df.iloc[:, 1].values

# Splitting the Dataset 
# ---
# We split our dataset into training and test sets. 80% = training set, while 20% = test set
# This means we will specify our parameter test_size below to have the value 0.2.
# ---
#

# Before we preform the split, we will import the train_test_split function
# which will help us perform this operation.
# ---
#
from sklearn.model_selection import train_test_split

# We then perform our split as show in the following line
# ---
# The test size of 0.2 indicates weâ€™ve used 20% of the data for testing. 
# random_state ensures reproducibility. For the output of train_test_split, 
# we get X_train, X_test, y_train, and y_test values.
# ---
#
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Training the Algorithm
# ---
# We are now ready to create our algorithm. This process is also referred to as 
# fitting in our model / fitting the regressor to the dataset.
# ---
#

# First, we import our linear regression function
# ---
#
from sklearn.linear_model import LinearRegression

# Then train the algorithm
# ---
#
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Making Predictions
# ---
# After training our algorithm, we can now make some preditions
# ---   
# 

# We create the y_pred variable, which will be an array (numpy array) 
# that contains all the predicted values for the input values in the X_test series
# ---
#
y_pred = regressor.predict(X_test)

# Next, we compare actual output values for X_test with the predicted values
# ---
#
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df

# We will note from the output that though our model is not very precise, 
# the predicted percentages are sometimes close to the actual ones.



# Finally, we evaluate our model performance
# --- 
# The final step is to evaluate the performance of algorithm. 
# Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors.
# ---  
#
from sklearn import metrics 
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

# Observation
# ---
# Our root mean square 2.74 is less than 10% the mean of y which was 49. 
# This would mean our model was fairly accurate.
# ---

"""#### <font color="green">Challenge 1</font>"""

# Challenge 
# ---
# Create a regression model that predicts salary based on years of experience.
# ---
# Dataset url = http://bit.ly/SLRSalaryDataset
# ---
# loading the data 
df = pd.read_csv("http://bit.ly/SLRSalaryDataset")
# check the data set dimensions rows and columns
df.shape
# preview the data set
df.head()
#describe the statistical summary
df.describe()
#check for missing values in the data set
df.isnull().sum()

#exploratory analysis within the given data set, check for relationship within the variables
df.plot(x='YearsExperience', y='Salary', style='
')
plt.title(' Years of experience vs Salary')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()
#positive correlation 
#divide the attributes
X = df.iloc[:, :-1].values
y = df.iloc[:, 1].values
#split the data set, import the train_test_split function
from sklearn.model_selection import train_test_split
#perform our split as show in the following line
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
#train the algorithm
#import the linear regression function
from sklearn.linear_model import LinearRegression
#train the algorithm
regressor = LinearRegression()
regressor.fit(X_train, y_train)
#making the predictions
y_pred = regressor.predict(X_test)
#compare the actual values and the predicted values
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df
#evaluation of the model perfomance
from sklearn import metrics 
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
df.describe()

"""## 2. Multiple Linear Regression

#### <font color="blue">Example 2</font>

##### <font color="blue">Example</font>
"""

# Example
# --- 
# Question: Create a multiple linear regression model to predict 
# the weight of fish given the following dataset.
# ---
# Dataset url = http://bit.ly/MRFishDataset
# ---
# OUR CODE GOES BELOW
#

"""##### Step 1. Loading our Data """

# Loading our dataset
#
fish_df = pd.read_csv('http://bit.ly/MRFishDataset')
fish_df.head()

"""##### Step 2. Checking the Data """

# Previewing the dataset
# 
fish_df.head()

# Previewing the statistical summary of our dataset
# 
fish_df.describe()

"""##### Step 3. Cleaning Our Data

So far from what we've observed from in dataset, we don't need to perform any cleaning.

##### Step 4. Performing Exploratory Analysis
"""

## Performing Exploratory Analysis
# ---
# This time we will plot a correlation matrix, to determine the relationships between the different variables.
# This matrix will give us a sense of how well the variables are correlated. By this we mean, whether an
# increase or decrease in variable affects the other variable. 
# To break this down further, the matrix will provide us with values between -1 and 1. If the value between
# two variables is closer to 1 i.e. > 0.5, then it means the variables are strongly correlated, have a positive linear 
# relationship and it also means that as one value increases the other increases.
# On the other hand, of the value is less than -0.5, it would mean that the variables are strongly correlated but 
# have a negative linear relationship. 
# If the value is 0 or < -0.5 or < 0.5 it means that the variables don't have any relationship with each other.
# ---
# Understanding whether our label i.e. Weight has a high correlation with other variables, helps us to deal 
# with the concept of Multicollinearity which would weaken the statistical power of your regression model. 
# For now, Multicollinearity is a topic beyond the scope of our session. However, it is usually important to 
# note that having highly correlated variables within our dataset weakens our model.
# ---
#
corrMatrix = fish_df.corr()
corrMatrix

# We can plot a visualisation of the matrix for better clarity
# --- 
# 
import seaborn as sns

# We define how big we want our visualisation
# 
plt.figure(figsize=(10, 10)) 

# Creating our visualisation
# 
sns.heatmap(corrMatrix, annot = True)

"""##### Step 5. Implementation and Evaluation

 
"""

# We will now quickly prepare our dataset so that we can fit our model
# ---
# We divide our data into attributes and labels. 
# This time, we use column names for creating an attribute set and label.
# ---
# 
X = fish_df[['Length1', 'Length2', 'Length3',
       'Height', 'Width']]
y = fish_df['Weight']

# We then Split our datset
# ---
# We split our dataset into training and test sets. 80% = training set, while 20% = test set
# ---
#

# Firstly, importing our train_test_split function
# ---
#
from sklearn.model_selection import train_test_split

# Performing our split
# ---
# 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Training the Algorithm
# ---
# 

# Firstly, importing our linear regression function
# ---
#
from sklearn.linear_model import LinearRegression

# Training the algorithm
# ---
# 
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Making Predictions
# ---
# After training our algorithm, we can now make some preditions
# ---
# We create the y_pred variable, which will contain 
# all the predicted values for the input values in the X_test series
# ---
#
y_pred = regressor.predict(X_test)

# Next, we compare actual output values for X_test with the predicted values
# This should also give us a sense of how our model performed during prediction
# ---
#
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df.sample(10)

# Finally, we evaluate our model performance
# --- 
# The final step is to evaluate the performance of algorithm  
# Root Mean Squared Error (RMSE). This is the square root of the mean of the squared errors.
# ---  
#
from sklearn import metrics 
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

# Observation
# ---
# We can see that the value of root mean squared error is 161.33, 
# which is greater than 10% of the mean value of the weight of 398. 
# This means that our algorithm fairly accurate and can make reasonably good predictions.
# There are many factors that may have contributed to this inaccuracy, a few of which are :
# 1. Need more data: Only one year worth of data isn't that much, 
#    whereas having multiple years worth could have helped us improve the accuracy quite a bit.
# 2. Bad assumptions: We made the assumption that this data has a linear relationship, 
#    but that might not be the case. Visualizing the data may help you determine that. 
# 3. Poor features: The features we used may not have had a high enough correlation 
#    to the values we were trying to predict.

"""#### <font color="green">Challenge 2</font>

**Problem Statement**

A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.

They have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market.

**Business Goal**

We are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.

Problem Source [Kaggle](https://www.kaggle.com/hellbuoy/car-price-prediction)
"""

# Challenge 
# ---
# Create a model to predict the price of cars given the following dataset.
# ---
# Dataset url = http://bit.ly/CarPriceDataset
# ---
# OUR CODE GOES BELOW
#

"""## 3. K-Nearest Neighbor (KNN) Regression

#### <font color="blue">Example 3</font>

##### <font color="blue">Example</font>
"""

# Example
# ---
# Question: Using the KNN Algorithm, create a regression model to 
# predict the weight of fish given the following dataset.
# ---
# Dataset url = http://bit.ly/FishDatasetClean
# NB: This dataset is clean version of the one 
# we used in the multiple regression example above.
# ---
# OUR CODE GOES BELOW
#

"""##### Step 1. Loading our Data """

# Reading our Dataset
# ---
# 
df = pd.read_csv('http://bit.ly/FishDatasetClean')
df.head()

# Previwing our dataset
# 
df.describe()

"""##### Step 2, 3, 4: Checking, Cleaning, Exploratory Analysis and have already been performed on our dataset in example 2

##### Step 5. Implementation and Evaluation
"""

# We will now quickly prepare our dataset so that we can fit our model
# ---
# We divide our data into attributes and labels. 
# This time, we use column names for creating an attribute set and label.
# ---
# 
X = df[['Length1', 'Length2', 'Length3',
       'Height', 'Width']]
y = df['Weight']

# We then Split our datset
# ---
# We split our dataset into training and test sets. 80% = training set, while 20% = test set
# ---
#

# Firstly, importing our train_test_split function
# ---
#
from sklearn.model_selection import train_test_split

# Performing our split
# ---
# 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Performing feature scaling
# ---
#
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()

X_train = sc_X.fit_transform(X_train)
X_test = sc_y.fit_transform(X_test)

# Applying our algorithm
# ---
# 
# Firstly, importing our KNN regression function
# ---
# 
from sklearn.neighbors import KNeighborsRegressor
  
# We initialize our algorithm with one parameter, i.e. n_neigbours. 
# This is basically the value for the K. 
# There is no ideal value for K and it is selected after testing and evaluation, 
# however to start out, we can use 5  as it is the most commonly used value for KNN algorithm.
# ---
#  
regressor = KNeighborsRegressor(5)
regressor.fit(X_train, y_train)

# Making predictions using our model
# ---
#  
y_pred = regressor.predict(X_test)

# Next, we compare actual output values for X_test with the predicted values
# ---
#
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df.sample(10)

# Finally, we evaluate the algorithm
# --- 
# The final step is to evaluate the performance of algorithm.
# Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors.
# ---  
#  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

"""#### <font color="green">Challenge 3</font>

##### <font color="green">Challenge</font>
"""

# Challenge 
# ---
# Create a regression model given the following dataset.
# ---
# Dataset url = http://bit.ly/RealEstateDataset2
# Hint: For ease of use of this dataset, drop the No. and date columns
# ---
# OUR CODE GOES BELOW
#

"""## 4. Decision Trees Regression

#### <font color="blue">Example 4</font>

##### <font color="blue">Example</font>
"""

# Example
# --- 
# Questions: Create a decision tree regression model using the following dataset.
# ---
# Dataset url = http://bit.ly/FishDatasetClean
# NB: This dataset is clean version of the one 
# we used in the multiple regression example above.
# ---
# OUR CODE GOES BELOW
#

"""##### Step 1. Loading our Data """

# Reading our data
# ---
# 
df = pd.read_csv('http://bit.ly/FishDatasetClean')
df.head()

# Describing our dataset
# ---
# 
df.describe()

"""##### Step 2, 3, 4: Checking, Cleaning, Exploratory Analysis and have already been performed on our dataset.

##### Step 5. Implementation and Evaluation
"""

# Let's now split our dataset
# ---
# 
# Firstly, importing our train_test_split function
# ---
#
from sklearn.model_selection import train_test_split

X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]
y = df['Weight']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)

# Lets now train our algorithm
# ---
#  

from sklearn.tree import DecisionTreeRegressor 

# Create the Decision Tree regressor object here.
# We only use the parameter random_state parameter with value 0
# ---
# 
regressor = DecisionTreeRegressor(random_state=0)
regressor.fit(X_train, y_train)

# Making predictions using our model
# ---
#  
y_pred = regressor.predict(X_test)

# Next, we compare actual output values for X_test with the predicted values
# ---
#
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df.sample(10)

# Finally, we evaluating the algorithm
# --- 
# The final step is to evaluate the performance of algorithm  
# ---  
#  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

"""#### <font color="green">Challenge 4</font>

##### <font color="green">Challenge</font>
"""

# Challenge 
# ---
# Using decision trees, create a regression model using the following dataset.
# ---
# Dataset url = http://bit.ly/RealEstateDataset2
# ---
# OUR CODE GOES BELOW
#

"""## 5. Support Vector Regression

#### <font color="blue">Example 5</font>

##### <font color="blue">Example</font>
"""

# Example
# --- 
# Question: Using the Support Vector Regressor, create a regression model using the clean dataset below.
# ---
# Dataset url = http://bit.ly/FishDatasetClean
# ---
# OUR CODE GOES BELOW
#

"""##### Step 1. Loading our Dataset"""

# Loading our dataset
# ---
# 
df = pd.read_csv('http://bit.ly/FishDatasetClean')
df.head()

# Describing our dataset
# ---
# 
df.describe()

"""##### Step 2, 3, 4: Checking, Cleaning, Exploratory Analysis and have already been performed on our dataset.

##### Step 5. Implementation and Evaluation
"""

# Splitting our dataset
# ---
# Weâ€™ll create the X and y variables by taking them from the dataset and
# ---
# 
X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]
y = df['Weight']

# We split our dataset as shown below
# ---
# The test size of 0.4 indicates weâ€™ve used 40% of the data for testing. 
# Random_state ensures reproducibility.
# ---
#
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)

# Performing feature scaling
# ---
#
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()

X_train = sc_X.fit_transform(X_train)
X_test = sc_y.fit_transform(X_test)

# Fitting in our model / Training our Algorithm / Fit the regressor to the scaled dataset
# ---
# We use the kernel type: 'linear'.
# Other kernel types: rbf, polynomial, sigmoid
# 

# We first import our model from sklearn.svm
# ---
# 
from sklearn.svm import SVR

# Then perform our training/fitting
# ---
#
regressor = SVR(kernel='linear')
regressor.fit(X_train, y_train)

# Making predictions using our model
# ---
#  
y_pred = regressor.predict(X_test)

# Finally, we evaluate our model
# --- 
# The final step is to evaluate the performance of algorithm  
# ---  
#  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

"""#### <font color="green">Challenge 5</font>

##### <font color="green">Challenge</font>
"""

# Challenge 
# ---
# Using SVM, create a regression model to predict price using the given dataset.
# Once you do so, determine which models is better
# ---
# Dataset url = http://bit.ly/RealEstateDataset2
# ---
# OUR CODE GOES BELOW
#